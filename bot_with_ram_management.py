#!/usr/bin/env python3
"""
Link Extractor ‡¶¨‡¶ü - Requests + BeautifulSoup (‡¶ï‡ßã‡¶®‡ßã Playwright ‡¶®‡ßá‡¶á)
‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶¨‡ßá Render ‡¶è
"""

import telebot
import requests
from bs4 import BeautifulSoup
import time
import psutil
import os
import gc
import re

# ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ Bot Token
BOT_TOKEN = "8348394510:AAHN41D99X35uVUi-7uAII4IECOzxB-EB3Q"

# Bot ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®
bot = telebot.TeleBot(BOT_TOKEN)

# RAM ‡¶∏‡ßÄ‡¶Æ‡¶æ (MB)
RAM_LIMIT = 500
RAM_CLEANUP_THRESHOLD = 450

# Headers
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

# Progress messages
progress_messages = [
    "‚è≥ savelinks ‡¶™‡ßá‡¶ú ‡¶ñ‡ßÅ‡¶≤‡¶õ‡¶ø...",
    "üîç ‡¶π‡ßã‡¶∏‡ßç‡¶ü‡¶ø‡¶Ç ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶ñ‡ßÅ‡¶Å‡¶ú‡¶õ‡¶ø...",
    "üìÑ ‡¶π‡ßã‡¶∏‡ßç‡¶ü‡¶ø‡¶Ç ‡¶™‡ßá‡¶ú ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶õ‡¶ø...",
    "üñ±Ô∏è ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶ñ‡ßÅ‡¶Å‡¶ú‡¶õ‡¶ø...",
    "‚è±Ô∏è ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶õ‡¶ø...",
    "‚úÖ ‡¶™‡ßç‡¶∞‡¶æ‡¶Ø‡¶º ‡¶∂‡ßá‡¶∑...",
]


def get_memory_usage():
    """‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® RAM ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ (MB)"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024


def cleanup_memory():
    """RAM ‡¶™‡¶∞‡¶ø‡¶∏‡ßç‡¶ï‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®"""
    gc.collect()
    time.sleep(0.3)


def check_and_cleanup_ram():
    """RAM ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶® ‡¶è‡¶¨‡¶Ç ‡¶™‡ßç‡¶∞‡¶Ø‡¶º‡ßã‡¶ú‡¶®‡ßá ‡¶™‡¶∞‡¶ø‡¶∏‡ßç‡¶ï‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®"""
    current_ram = get_memory_usage()
    
    if current_ram > RAM_CLEANUP_THRESHOLD:
        print(f"‚ö†Ô∏è RAM high: {current_ram:.1f}MB - Cleaning up...")
        cleanup_memory()
        current_ram = get_memory_usage()
        print(f"‚úÖ RAM after cleanup: {current_ram:.1f}MB")
    
    if current_ram > RAM_LIMIT:
        print(f"üî¥ RAM CRITICAL: {current_ram:.1f}MB > {RAM_LIMIT}MB")
        return False
    
    return True


def extract_link(url):
    """‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®"""
    
    if "savelinks.me" not in url:
        return {
            "success": False,
            "error": "‚ùå savelinks.me URL ‡¶¶‡¶ø‡¶®"
        }
    
    try:
        # RAM ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®
        if not check_and_cleanup_ram():
            return {
                "success": False,
                "error": "‚ùå ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶æ‡¶∞ overload - ‡¶™‡¶∞‡ßá ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®"
            }
        
        # savelinks ‡¶™‡ßá‡¶ú ‡¶•‡ßá‡¶ï‡ßá ‡¶π‡ßã‡¶∏‡ßç‡¶ü‡¶ø‡¶Ç ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
        session = requests.Session()
        session.headers.update(HEADERS)
        
        response = session.get(url, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ‡¶∏‡¶¨ ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®
        hosting_url = None
        links = soup.find_all('a', href=True)
        
        # gdflix ‡¶ï‡ßá ‡¶Ö‡¶ó‡ßç‡¶∞‡¶æ‡¶ß‡¶ø‡¶ï‡¶æ‡¶∞ ‡¶¶‡¶ø‡¶®
        for link in links:
            href = link['href']
            if 'gdflix' in href:
                hosting_url = href
                break
        
        # ‡¶Ø‡¶¶‡¶ø gdflix ‡¶®‡¶æ ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º, ‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®
        if not hosting_url:
            for link in links:
                href = link['href']
                if 'hubcloud' in href or 'filepress' in href:
                    hosting_url = href
                    break
        
        if not hosting_url:
            return {
                "success": False,
                "error": "‚ùå ‡¶π‡ßã‡¶∏‡ßç‡¶ü‡¶ø‡¶Ç ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡¶®‡¶ø"
            }
        
        # ‡¶π‡ßã‡¶∏‡ßç‡¶ü‡¶ø‡¶Ç ‡¶™‡ßá‡¶ú ‡¶•‡ßá‡¶ï‡ßá ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
        response = session.get(hosting_url, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®
        download_link = None
        
        # ‡¶∏‡¶¨ ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®
        links = soup.find_all('a', href=True)
        
        for link in links:
            text = link.get_text(strip=True)
            href = link['href']
            
            # INSTANT DL ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®
            if 'INSTANT DL' in text and href.startswith('http'):
                download_link = href
                break
            
            # ‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï
            if href.startswith('http') and any(x in href for x in ['busycdn', 'r2.dev', 'pixeldrain']):
                download_link = href
                break
        
        # JavaScript ‡¶è ‡¶•‡¶æ‡¶ï‡¶æ ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®
        if not download_link:
            # Page source ‡¶è regex ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®
            page_source = response.text
            
            # busycdn ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®
            match = re.search(r'https://instant\.busycdn\.xyz/[a-f0-9:]+', page_source)
            if match:
                download_link = match.group(0)
            
            # r2.dev ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®
            if not download_link:
                match = re.search(r'https://pub-[a-f0-9]+\.r2\.dev/[^\s"\'<>]+', page_source)
                if match:
                    download_link = match.group(0)
            
            # pixeldrain ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®
            if not download_link:
                match = re.search(r'https://pixeldrain\.dev/u/[a-zA-Z0-9]+', page_source)
                if match:
                    download_link = match.group(0)
        
        session.close()
        cleanup_memory()
        
        if download_link:
            return {
                "success": True,
                "downloadLink": download_link
            }
        else:
            return {
                "success": False,
                "error": "‚ùå ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡¶®‡¶ø"
            }
    
    except requests.Timeout:
        return {
            "success": False,
            "error": "‚ùå Timeout - ‡¶™‡¶∞‡ßá ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®"
        }
    except Exception as e:
        cleanup_memory()
        return {
            "success": False,
            "error": f"‚ùå Error: {str(e)[:50]}"
        }


@bot.message_handler(commands=['start'])
def send_welcome(message):
    """‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡ßç‡¶ü ‡¶ï‡¶Æ‡¶æ‡¶®‡ßç‡¶°"""
    ram_usage = get_memory_usage()
    text = f"""üîó <b>Link Extractor ‡¶¨‡¶ü</b>

savelinks.me ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶™‡¶æ‡¶†‡¶æ‡¶®, ‡¶Ü‡¶Æ‡¶ø ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶¶‡ßá‡¶¨‡•§

<b>‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:</b>
<code>https://savelinks.me/view/IJRaLXbQ</code>

üìä <b>‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶æ‡¶∞ ‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶∏:</b>
RAM: {ram_usage:.1f}MB / {RAM_LIMIT}MB"""
    
    bot.reply_to(message, text, parse_mode="HTML")


@bot.message_handler(commands=['status'])
def send_status(message):
    """‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶∏ ‡¶ï‡¶Æ‡¶æ‡¶®‡ßç‡¶°"""
    ram_usage = get_memory_usage()
    status = "‚úÖ Good" if ram_usage < 400 else "‚ö†Ô∏è High" if ram_usage < 480 else "üî¥ Critical"
    
    text = f"""üìä <b>‡¶¨‡¶ü ‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶∏</b>

RAM: {ram_usage:.1f}MB / {RAM_LIMIT}MB
Status: {status}"""
    
    bot.reply_to(message, text, parse_mode="HTML")


@bot.message_handler(func=lambda message: True)
def handle_message(message):
    """‡¶∏‡¶¨ ‡¶¨‡¶æ‡¶∞‡ßç‡¶§‡¶æ ‡¶π‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡ßá‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®"""
    
    text = message.text
    
    # URL ‡¶ï‡¶ø ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®
    if not text or not ("savelinks.me" in text or "http" in text):
        return
    
    # ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç ‡¶Æ‡ßá‡¶∏‡ßá‡¶ú ‡¶™‡¶æ‡¶†‡¶æ‡¶®
    processing_msg = bot.send_message(
        message.chat.id,
        "‚è≥ " + progress_messages[0],
        parse_mode="HTML"
    )
    
    # ‡¶™‡ßç‡¶∞‡¶ó‡¶§‡¶ø ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®
    for i, prog_text in enumerate(progress_messages):
        try:
            bot.edit_message_text(
                "‚è≥ " + prog_text,
                message.chat.id,
                processing_msg.message_id,
                parse_mode="HTML"
            )
            time.sleep(1.2)
        except:
            pass
    
    # ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶ï‡¶∂‡¶® ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡ßÅ‡¶®
    try:
        result = extract_link(text)
        
        if result["success"]:
            response = f"""‚úÖ <b>‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶™‡ßá‡¶Ø‡¶º‡ßá‡¶õ‡¶ø!</b>

<code>{result['downloadLink']}</code>

üìã ‡¶ï‡¶™‡¶ø ‡¶ï‡¶∞‡ßá ‡¶¨‡ßç‡¶∞‡¶æ‡¶â‡¶ú‡¶æ‡¶∞‡ßá ‡¶™‡ßá‡¶∏‡ßç‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®‡•§"""
        else:
            response = result["error"]
        
        bot.edit_message_text(
            response,
            message.chat.id,
            processing_msg.message_id,
            parse_mode="HTML"
        )
    
    except Exception as e:
        bot.edit_message_text(
            f"‚ùå Error: {str(e)[:50]}",
            message.chat.id,
            processing_msg.message_id,
            parse_mode="HTML"
        )


if __name__ == "__main__":
    print("ü§ñ ‡¶¨‡¶ü ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡¶ø...")
    print(f"üìä RAM Limit: {RAM_LIMIT}MB")
    print(f"‚ö†Ô∏è Cleanup Threshold: {RAM_CLEANUP_THRESHOLD}MB")
    print("Ctrl+C ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßÅ‡¶®")
    
    try:
        bot.infinity_polling()
    except KeyboardInterrupt:
        print("\n‚úÖ ‡¶¨‡¶ü ‡¶¨‡¶®‡ßç‡¶ß ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá")
